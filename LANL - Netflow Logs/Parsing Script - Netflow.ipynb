{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading file into chunks of 10 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The most powerful machine available on Rivanna can hold up to 10 Million rows for data manipulations and we decided to read  the Netflow logs in chunks of 10 millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "os.chdir(\"/home/rk9cx/LANL/\")\n",
    "file = \"netflow_day-02.txt\"\n",
    "chunksize = 10000000\n",
    "header = ['Time', 'Duration','SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', 'DstPort','SrcPackets','DstPackets',\n",
    "         'SrcBytes','DstBytes']\n",
    "df_reader = pd.read_csv(file, chunksize = chunksize, header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing last 10 million rows for deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We noticed that records in the Netflow logs are cumulative in nature and we decided to deduplicate. In addition to this, we also noticed that the duplicates could slip between chunks, we are combatting this by storing the information for the last few rows and using it in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastrow = pd.DataFrame(columns = header)\n",
    "for cidx, df in enumerate(df_reader):\n",
    "    final = df\n",
    "    final.columns = header\n",
    "    #converting source and destination ports into string as required by the Pandas drop_duplicates function\n",
    "    final.SrcPort = final.SrcPort.astype(str)\n",
    "    final.DstPort = final.DstPort.astype(str)\n",
    "    #when reading the file in chunks, indices are inevitable, the command below removes the indices\n",
    "    final = pd.concat([lastrow, final])\n",
    "    final.reset_index(inplace = True)\n",
    "    final = final.drop([\"index\"], axis = 1)\n",
    "    #sorting the values to ensure that logs or recods in a sequence is arranged in ascending order\n",
    "    final = final.sort_values(['Time','SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', 'DstPort', 'DstBytes'], \n",
    "                              ascending=[True, True, True,True,True,True,True])\n",
    "    #removing duplicates whilst retaining the final/ last row using the timestamp and five tuples \n",
    "    final = final.drop_duplicates(subset = ['Time', 'SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', 'DstPort'], \n",
    "                       keep = 'last')\n",
    "    #saving all rows except the last 1 million\n",
    "    final[:-1000000].to_csv('parsed.csv', mode ='a', header = False, index = False)\n",
    "    #storing the last 1 million to be used for next iteration\n",
    "    lastrow = final.tail(1000000)\n",
    "#adding the lastrow from the iteration to the parsed.csv\n",
    "lastrow.to_csv('parsed.csv', mode ='a', header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the existence of duplicates (Time + five tuples) in the logs, its imperative that we check for if there are any duplicates that exist in the parsed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'parsed.csv'\n",
    "df_reader1 = pd.read_csv(file, chunksize = 10000000, index_col=False)\n",
    "for cidx, df in enumerate(df_reader1):\n",
    "    #number of rows of parsed file\n",
    "    p = df.shape[0]\n",
    "    header = [ 'Time', 'Duration','SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', 'DstPort','SrcPackets','DstPackets',\n",
    "         'SrcBytes','DstBytes']\n",
    "    c = df\n",
    "    c.reset_index(drop = True)\n",
    "    c.columns = header\n",
    "    c = c.sort_values(['Time', 'Duration','SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', 'DstPort'], ascending=[True,True, True, True,True,True,True])\n",
    "    c = c.drop_duplicates(subset = ['Time', 'SrcDevice', 'DstDevice', 'Protocol', 'SrcPort', 'DstPort'], \n",
    "                       keep = 'last')\n",
    "    #number of rows of parsed file after removing duplicates\n",
    "    q = c.shape[0]\n",
    "    #condition to check if there are any duplicates in the parsed file\n",
    "    print(p-q)\n",
    "    if p-q!=0:\n",
    "        print(cidx)\n",
    "        print(\"Quality Check Failed!\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
